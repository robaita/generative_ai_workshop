# Generative AI workshop
This repository demonstrates the process of fine-tuning large language models (LLMs) and hosting them on your local machine, specifically focusing on the LLAMA model. The guide covers the following:
> **[Hosting LLAMA on Your Laptop:](docs/hosting_ollama.md)** A step-by-step approach to running the LLAMA model locally with minimal resource requirements.  
> **[LAN Accessibility:](docs/ollama_command.md)** Making the model accessible across a local network (LAN).  
> **[Microservice Deployment:](docs/ollama_with_python.md)** How to serve the LLM as a REST API.  
> **[Multi-Modal LLM Examples:](docs/ollama_with_interface.md)** Demonstrating the integration of multiple modalities (e.g., text and image) to build powerful and versatile AI applications.  
> **[LLM Fine-Tuning:](docs/finetune_llm.md)** Instructions and examples for fine-tuning pre-trained models on custom dataset.  

## What is [Ollama](https://ollama.com/)
Ollama is an open-source framework designed to simplify the development and deployment of large language models (LLMs), particularly for tasks like fine-tuning, serving, and scaling models in real-world applications. Built with a focus on ease of use, flexibility, and performance, Ollama offers several key features that make it highly useful for both developers and researchers working with LLMs.

### Features of Ollama:
> **Easy Model Hosting:** Ollama makes it easy to host large language models, such as LLAMA, on local machines or servers, enabling users to interact with models without needing complex infrastructure.  
> **Fine-Tuning Capabilities:** It allows fine-tuning of pre-trained LLMs on domain-specific data, enabling the creation of highly specialized models tailored for particular tasks or industries.  
> **Efficient Inference:** Ollama provides optimized inference pipelines that ensure fast and efficient model predictions, even with large models.  
> **Multi-Modal Support:** Ollama includes support for integrating text, image, and other modalities, enabling the creation of multi-modal models that can handle diverse inputs and outputs.  
> **Microservice Integration:** Ollama is designed to be easily served as a microservice, making it ideal for building scalable and distributed AI systems.  
> **LAN Accessibility:** It allows models to be deployed in a local network, ensuring that AI models are accessible remotely from multiple devices within the same network. 
> **Minimal Resource Usage:** Ollama is optimized for use on personal machines, reducing the need for expensive cloud infrastructure while still providing powerful AI capabilities.  

### [Hero Vired Generative AI Study Materials](docs/hero_vired.md)


